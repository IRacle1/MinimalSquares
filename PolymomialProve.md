# Доказательство линейности МНК для линейной комбинации функций

Дана функция $f(x) = a_1 f_1(x) + a_2 f_2(x) +  \dots + a_n f_n(x)$

Где $a_1, a_2 \dots , a_n$ - параметры, а $f_1, f_2 \dots , f_n$ - произвольные функции.

*В дальшейнем я буду называть такую функцию **линейной***

Задача в том, что бы найти линейный алгоритм решения для минимизации отклонения функции $f(x)$ от дискретных данных используя метод наименьших квадратов.

Метод наименьших квадратов заключается в минимизации суммы:

$$\sum_{i=1}^{k} (y_i - f(x_i))^2$$

Эта сумма для фиксированного набора точек будет зависить только от выбранных параметров, можно написать:

$$F(a_1, a_2, \dots , a_n) = \sum_{i=1}^{k} (y_i - f(x_i))^2$$

Задача сводится к минимизации функции нескольких переменных $F$, поэтому можно применить необходимый признак экстремума для нахождения её минимума, а для этого нужно найти все частные производные по аргументам, в целом не составит труда найти производную в общем виде:

$$F'_{a_j} = (\sum_{i=1}^{k} (y_i - f(x_i))^2)' = 0$$

По свойствам производной заносим её в сумму, и дифференцируем квадрат

$$\sum_{i=1}^{k} 2 * (y_i - f(x_i)) * (y_i - f(x_i))' = 0$$

$y_i$ - постоянный множитель при дифференцировании + выносим двойку за сумму и сокращаем

$$\sum_{i=1}^{k} (y_i - f(x_i)) * (-f'_{a_j}(x_i)) = 0$$

Производная функции $f$ по произвольному $a_n$ очень простая:

$$f'_{a_j}(x) = f_j(x)$$

Подставляем и раскрываем скобки

$$\sum_{i=1}^{k} (y_i - f(x_i)) * (-f_j(x_i))= 0$$

$$\sum_{i=1}^{k} (f(x_i) * f_j(x_i) - y_i * f_j(x_i))= 0$$

Распределяем суммы и переносим сумму с игреком

$$\sum_{i=1}^{k} f(x_i) * f_j(x_i) = \sum_{i=1}^{k} y_i * f_j(x_i)$$

Тепер вместо $f(x_i)$ напишем раскрытый вид начальной функции 

$$\sum_{i=1}^{k} (a_1 f_1(x_i) + a_2 f_2(x_i) + \dots) * f_j(x_i) = \sum_{i=1}^{k} y_i * f_j(x_i)$$

Опять перераспределим суммы

$$\sum_{i=1}^{k} a_1 f_1(x_i) * f_j(x_i) + \sum_{i=1}^{k} a_2 f_2(x_i) * f_j(x_i) + \dots + \sum_{i=1}^{k} a_n f_n(x_i) * f_j(x_i) = \sum_{i=1}^{k} y_i * f_j(x_i)$$

Вынесем коефициенты $a$ за суммы

$$a_1 \sum_{i=1}^{k} f_1(x_i) * f_j(x_i) + a_2 \sum_{i=1}^{k} f_2(x_i) * f_j(x_i) + \dots + a_n \sum_{i=1}^{k}f_n(x_i) * f_j(x_i) = \sum_{i=1}^{k} y_i * f_j(x_i)$$

В итоге мы получили общее выражение для частной производной функции. Теперь можно записать все частные производные для необходимого признака экстремума ввиде системы.

$$
\begin{cases} 
a_1 \sum_{i=1}^{k} f_1(x_i) * f_1(x_i) + a_2 \sum_{i=1}^{k} f_2(x_i) * f_1(x_i) + \dots + a_n \sum_{i=1}^{k}f_n(x_i) * f_1(x_i) = \sum_{i=1}^{k} y_i * f_1(x_i)
\\
a_1 \sum_{i=1}^{k} f_1(x_i) * f_2(x_i) + a_2 \sum_{i=1}^{k} f_2(x_i) * f_2(x_i) + \dots + a_n \sum_{i=1}^{k}f_n(x_i) * f_2(x_i) = \sum_{i=1}^{k} y_i * f_2(x_i)
\\
\vdots
\\
a_1 \sum_{i=1}^{k} f_1(x_i) * f_n(x_i) + a_2 \sum_{i=1}^{k} f_2(x_i) * f_n(x_i) + \dots + a_n \sum_{i=1}^{k}f_n(x_i) * f_n(x_i) = \sum_{i=1}^{k} y_i * f_n(x_i)
\end{cases}
$$

Теперь видно что эту систему можно представить как матричное умножение

$$AX=B$$

Где

$$A=
\begin{pmatrix}
\sum_{i=1}^{k} f_1(x_i) * f_1(x_i) & \sum_{i=1}^{k} f_2(x_i) * f_1(x_i) & \dots & \sum_{i=1}^{k} f_n(x_i) * f_1(x_i)\\
\sum_{i=1}^{k} f_1(x_i) * f_2(x_i) & \sum_{i=1}^{k} f_2(x_i) * f_2(x_i) & \dots & \sum_{i=1}^{k} f_n(x_i) * f_2(x_i)\\
\vdots &\vdots & \ddots & \vdots \\
\sum_{i=1}^{k} f_1(x_i) * f_n(x_i) & \sum_{i=1}^{k} f_2(x_i) * f_n(x_i) & \dots & \sum_{i=1}^{k} f_n(x_i) * f_n(x_i)\\
\end{pmatrix}$$

*проще говоря, $A_{ij} = \sum_{i=1}^{k} f_i(x_i) * f_j(x_i)$*

$$B=
\begin{pmatrix}
\sum_{i=1}^{k} y_i * f_1(x_i) \\
\sum_{i=1}^{k} y_i * f_2(x_i) \\
\vdots \\
\sum_{i=1}^{k} y_i * f_n(x_i) \\
\end{pmatrix}$$

*проще говоря, $B_{i} = \sum_{i=1}^{k} y_i * f_i(x_i)$*

$$X=
\begin{pmatrix}
a_1 \\
a_2 \\
\vdots \\
a_n \\
\end{pmatrix}$$

$X$ - вектор переменных, которые нам и нужно найти, просто решив матричное уравление.

Таким образом, любую функцию такого вида(или приводимую к такому виду) можно минимизировать с помощью линейного МНК, только вычислив систему линейных уравнений.

## Обобщение с $y$

Были рассмотрены линейные функции вида $y = a_1 f_1(x) + a_2 f_2(x) + \dots + a_n f_n(x)$, но для линейного МНК не обязательно выражать функцию через чистый $y$, зависимости вроде $g(y) = \dots $ тоже можно линейно минимизировать, просто заменив применив функцию $g$ к $y$ координатам точек. Поменяется лишь свободный вектор $B$:

$$B=
\begin{pmatrix}
\sum_{i=1}^{k} y_i * f_1(x_i) \\
\sum_{i=1}^{k} y_i * f_2(x_i) \\
\vdots \\
\sum_{i=1}^{k} y_i * f_n(x_i) \\
\end{pmatrix} \rightarrow \begin{pmatrix}
\sum_{i=1}^{k} g(y_i) * f_1(x_i) \\
\sum_{i=1}^{k} g(y_i) * f_2(x_i) \\
\vdots \\
\sum_{i=1}^{k} g(y_i) * f_n(x_i) \\
\end{pmatrix}$$

Такие зависимости сами по себе нет смысла минимизировать. Но есть нелинейные сложные функции, которые можно будет свести к линейным.

## Примеры

### Экспоненциальная функция

$$y = b * e^{ax}$$

Для приведения этой функции к линейной, достаточно прологарифмировать левую и правую части.

$$\ln(y) = \ln(b * e^{ax})$$
$$\ln(y) = \ln(e^{ax}) + \ln(b)$$
$$\ln(y) = ax + \ln(b)$$

Эту зависимость уже можно минимизировать, сделав замену $b_1 = ln(b)$

$$\ln(y) = ax + b_1$$

П потом выразить оригинальные параметры через обратную замену *(В данном случае $b = e^{b_1}$)*

### Синусоида

$$y = a\sin(Kx + b) + c$$

Можно раскрыть синус суммы:

$$y = a (\sin(Kx)\cos(b) + \cos(Kx)\sin(b)) + c$$

$$y = a \cos(b) * \sin(Kx) + a \sin(b) * \cos(Kx) + c$$

Сделав замену 

$$a_1 = a\cos(b); a_2 = a \sin(b)$$

Получим выражение:

$$y = a_1 * \sin(Kx) + a_2 * \cos(Kx) + c$$

После линейного МНК, выражаем оригинальные $a$ и $b$:

$$a = \sqrt{a_1^2 + a_2^2}$$
$$b = \arctg(a_2/a_1)$$

### $y = e^{f(x)}$

Для примера можно принять функцию $y = e^{ax^2 + bx + c}$, хотя такой подход применим к любым подобным функциям, где в показателе любое линейное выражение.

Для приведения функции к линейной достаточно просто прологарифмировать:

$$\ln y = \ln(e^{ax^2 + bx + c})$$

$$\ln y = ax^2 + bx + c$$

Даже параметры выражать не нужно.